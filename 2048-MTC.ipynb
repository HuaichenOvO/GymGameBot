{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_2048\n",
    "import numpy as np\n",
    "import math\n",
    "import copy  # For deep copying the environment\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MCTS Node definition\n",
    "class MCTSNode:\n",
    "    def __init__(self, state, action=None, parent=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.visits = 0\n",
    "        self.value = 0\n",
    "        self.action = action  # Store the action taken to reach this state\n",
    "\n",
    "    def add_child(self, child_state, action):\n",
    "        child_node = MCTSNode(child_state, action=action, parent=self)\n",
    "        self.children.append(child_node)\n",
    "        return child_node\n",
    "\n",
    "    def update(self, reward):\n",
    "        self.visits += 1\n",
    "        self.value += reward\n",
    "\n",
    "# MCTS Class definition\n",
    "class MCTS:\n",
    "    def __init__(self, env, exploration_weight=1.4):\n",
    "        self.env = env\n",
    "        self.exploration_weight = exploration_weight\n",
    "\n",
    "    def select(self, node):\n",
    "        # UCB (Upper Confidence Bound) score calculation for selecting the best child\n",
    "        def ucb_score(n):\n",
    "            if n.visits == 0:\n",
    "                return float('inf')  # Assign infinite value if not visited\n",
    "            return (n.value / n.visits) + self.exploration_weight * math.sqrt(math.log(node.visits) / n.visits)\n",
    "\n",
    "        return max(node.children, key=ucb_score)\n",
    "\n",
    "    def expand(self, node):\n",
    "        actions = range(self.env.action_space.n)\n",
    "        for action in actions:\n",
    "            # Create a copy of the environment to simulate the current state\n",
    "            env_copy = copy.deepcopy(self.env)\n",
    "            env_copy.reset()\n",
    "            env_copy.unwrapped.board = copy.deepcopy(node.state)  # Set the board state for the copied environment\n",
    "\n",
    "            next_state, reward, done, _ = env_copy.step(action)\n",
    "            if not done:  # Check if the resulting state is terminal\n",
    "                node.add_child(copy.deepcopy(env_copy.unwrapped.board), action)\n",
    "\n",
    "    def simulate(self, node):\n",
    "        # Create a new environment copy to simulate from the node state\n",
    "        env_copy = copy.deepcopy(self.env)\n",
    "        env_copy.reset()\n",
    "        env_copy.unwrapped.board = copy.deepcopy(node.state)  # Set the board state for the copied environment\n",
    "\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = env_copy.action_space.sample()  # Randomly choose an action for the simulation\n",
    "            _, reward, done, _ = env_copy.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        return total_reward\n",
    "\n",
    "    def backpropagate(self, node, reward):\n",
    "        # Backpropagate the reward through the tree\n",
    "        current_node = node\n",
    "        while current_node:\n",
    "            current_node.update(reward)\n",
    "            current_node = current_node.parent\n",
    "\n",
    "    def search(self, root_state, iterations=100):\n",
    "        root_node = MCTSNode(root_state)\n",
    "\n",
    "        for _ in range(iterations):\n",
    "            node = root_node\n",
    "\n",
    "            # Use a deep copy of the environment to simulate the state\n",
    "            env_copy = copy.deepcopy(self.env)\n",
    "            env_copy.reset()\n",
    "            env_copy.unwrapped.board = copy.deepcopy(root_state)  # Set the board state for the copied environment\n",
    "\n",
    "            # Selection: Navigate to the most promising child node\n",
    "            while node.children:\n",
    "                node = self.select(node)\n",
    "\n",
    "            # Expansion: Expand if the game is not over\n",
    "            actions = range(self.env.action_space.n)\n",
    "            done = False\n",
    "            for action in actions:\n",
    "                next_state, reward, done, _ = env_copy.step(action)\n",
    "                if not done:\n",
    "                    break\n",
    "\n",
    "            if not done:\n",
    "                self.expand(node)\n",
    "\n",
    "            # Simulation: Simulate to get a reward\n",
    "            reward = self.simulate(node)\n",
    "\n",
    "            # Backpropagation: Backpropagate the reward through the tree\n",
    "            self.backpropagate(node, reward)\n",
    "\n",
    "        # Choose the best action based on the most visits\n",
    "        if root_node.children:\n",
    "            return max(root_node.children, key=lambda n: n.visits)\n",
    "        else:\n",
    "            return None  # No children were generated, likely a terminal state\n",
    "\n",
    "# Initialize the environment and MCTS agent\n",
    "env = gym.make('2048-v0')\n",
    "mcts = MCTS(env)\n",
    "\n",
    "# Evaluator class definition\n",
    "class Evaluator:\n",
    "    def __init__(self, env, mcts, episodes=10):\n",
    "        self.env = env\n",
    "        self.mcts = mcts\n",
    "        self.episodes = episodes\n",
    "\n",
    "    def evaluate(self):\n",
    "        scores = []\n",
    "        highest_tiles = []\n",
    "        tile_achievements = {2048: 0, 4096: 0, 8192: 0}\n",
    "\n",
    "        for episode in range(self.episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            total_score = 0\n",
    "\n",
    "            while not done:\n",
    "                # Use MCTS to determine the best action for each state\n",
    "                best_action_node = self.mcts.search(state, iterations=100)\n",
    "                if best_action_node is None:\n",
    "                    break  # If no action is found, terminate the episode\n",
    "\n",
    "                action = best_action_node.action\n",
    "\n",
    "                # Execute the chosen action in the actual environment\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                total_score += reward\n",
    "\n",
    "            # Collect the final score and highest tile achieved\n",
    "            scores.append(total_score)\n",
    "            highest_tile = np.max(self.env.unwrapped.board)\n",
    "            highest_tiles.append(highest_tile)\n",
    "\n",
    "            # Update tile achievement counts\n",
    "            for tile in tile_achievements.keys():\n",
    "                if highest_tile >= tile:\n",
    "                    tile_achievements[tile] += 1\n",
    "\n",
    "        # Calculate and print metrics\n",
    "        average_score = np.mean(scores) if scores else 0\n",
    "        win_rate_2048 = tile_achievements[2048] / self.episodes if self.episodes > 0 else 0\n",
    "        win_rate_4096 = tile_achievements[4096] / self.episodes if self.episodes > 0 else 0\n",
    "        win_rate_8192 = tile_achievements[8192] / self.episodes if self.episodes > 0 else 0\n",
    "        highest_tile_achieved = max(highest_tiles) if highest_tiles else 0\n",
    "\n",
    "        print(f\"Average Score: {average_score}\")\n",
    "        print(f\"Win Rate for 2048 Tile: {win_rate_2048 * 100:.2f}%\")\n",
    "        print(f\"Win Rate for 4096 Tile: {win_rate_4096 * 100:.2f}%\")\n",
    "        print(f\"Win Rate for 8192 Tile: {win_rate_8192 * 100:.2f}%\")\n",
    "        print(f\"Highest Tile Achieved in All Episodes: {highest_tile_achieved}\")\n",
    "\n",
    "        return {\n",
    "            \"scores\": scores,\n",
    "            \"highest_tiles\": highest_tiles,\n",
    "            \"average_score\": average_score,\n",
    "            \"win_rate_2048\": win_rate_2048,\n",
    "            \"win_rate_4096\": win_rate_4096,\n",
    "            \"win_rate_8192\": win_rate_8192,\n",
    "            \"highest_tile\": highest_tile_achieved\n",
    "        }\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluator = Evaluator(env, mcts, episodes=10)\n",
    "evaluation_results = evaluator.evaluate()\n",
    "\n",
    "# Extract scores and highest tiles for visualization\n",
    "scores = evaluation_results['scores']\n",
    "highest_tiles = evaluation_results['highest_tiles']\n",
    "\n",
    "# Plotting the results\n",
    "# Plotting average score per episode\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(scores)), scores, label=\"Score per Episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Score Progression Over Episodes\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting tile achievement distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(highest_tiles, bins=[0, 512, 1024, 2048, 4096, 8192, 16384], edgecolor='black')\n",
    "plt.xlabel(\"Highest Tile\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Highest Tile Achieved\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
